{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6IeWLgzXP+mPjUwctSYfp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vivek-Kasturi/credit-card-fraud-detection/blob/colab-pipeline/fraud_detection_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5DwEZJ7o-e-",
        "outputId": "2784036d-0dae-4b9a-d5ee-9bc0ae8587a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows,cols: (284807, 31)\n",
            "Class\n",
            "0    284315\n",
            "1       492\n",
            "Name: count, dtype: int64\n",
            "Fraud ratio: 0.001727485630620034\n",
            "Logistic AUC: 0.968 AUPR: 0.704\n",
            "\n",
            "Classification report (threshold=0.5):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9998    0.9788    0.9892     85295\n",
            "           1     0.0669    0.8784    0.1244       148\n",
            "\n",
            "    accuracy                         0.9786     85443\n",
            "   macro avg     0.5334    0.9286    0.5568     85443\n",
            "weighted avg     0.9982    0.9786    0.9877     85443\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/tsa/base/tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
            "  return get_prediction_index(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used SARIMAX forecasting (statsmodels).\n",
            "All outputs saved to /content/outputs\n",
            "Top 5 days flagged as anomalies (by mean+3std):\n",
            "Empty DataFrame\n",
            "Columns: [date, total_transactions, actual_frauds, expected_frauds, mean_pred_prob, fraud_rate, anomaly_mean3std, anomaly_p99_rate, frauds_7d, anomaly_7d_burst]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# Full pipeline: classification + detection of fraud times + 1-year forecast\n",
        "# Run in Colab or local Python (requirements below)\n",
        "# Requirements: pandas, numpy, scikit-learn, matplotlib, seaborn, statsmodels (optional), pmdarima (optional)\n",
        "# (In Colab: pip install pmdarima if you want auto ARIMA. statsmodels often preinstalled.)\n",
        "\n",
        "import os, warnings, math\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, classification_report\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "# Params\n",
        "CSV_PATH = '/content/creditcard.csv'  # update path if not in same folder\n",
        "OUTPUT_DIR = '/content/outputs'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "ANCHOR_DATE = pd.to_datetime('2013-01-01')   # arbitrary anchor date for Time -> datetime\n",
        "FORECAST_DAYS = 365\n",
        "\n",
        "# 1) Load & quick checks\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Rows,cols:\", df.shape)\n",
        "print(df['Class'].value_counts())\n",
        "print(\"Fraud ratio:\", df['Class'].mean())\n",
        "\n",
        "# 2) Preprocessing: scale Time and Amount\n",
        "X = df.drop('Class', axis=1).copy()\n",
        "y = df['Class'].copy()\n",
        "scaler = StandardScaler()\n",
        "X[['Time','Amount']] = scaler.fit_transform(X[['Time','Amount']])\n",
        "\n",
        "# 3) Train-test split (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)\n",
        "\n",
        "# 4) Fit a solid, fast baseline: LogisticRegression with class_weight\n",
        "clf = LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 5) Evaluate on test\n",
        "proba_test = clf.predict_proba(X_test)[:,1]\n",
        "auc = roc_auc_score(y_test, proba_test)\n",
        "aupr = average_precision_score(y_test, proba_test)\n",
        "print(\"Logistic AUC:\", round(auc,4), \"AUPR:\", round(aupr,4))\n",
        "print(\"\\nClassification report (threshold=0.5):\")\n",
        "print(classification_report(y_test, (proba_test>=0.5).astype(int), digits=4))\n",
        "\n",
        "# Save PR curve\n",
        "precision, recall, thresh = precision_recall_curve(y_test, proba_test)\n",
        "plt.figure(figsize=(6,4)); plt.plot(recall, precision, lw=2); plt.xlabel('Recall'); plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall curve (Logistic)'); plt.grid(True)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR,'pr_curve_logistic.png'), bbox_inches='tight'); plt.close()\n",
        "\n",
        "# 6) Fit on full dataset for scoring every transaction\n",
        "clf_full = LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear')\n",
        "clf_full.fit(X, y)\n",
        "df['pred_proba'] = clf_full.predict_proba(X)[:,1]\n",
        "\n",
        "# 7) Convert Time to datetime (relative) & aggregate by day\n",
        "df['datetime'] = ANCHOR_DATE + pd.to_timedelta(df['Time'], unit='s')\n",
        "df['date'] = pd.to_datetime(df['datetime'].dt.date)\n",
        "daily = df.groupby('date').agg(\n",
        "    total_transactions = ('Class','count'),\n",
        "    actual_frauds = ('Class','sum'),\n",
        "    expected_frauds = ('pred_proba','sum'),   # sum of predicted probabilities ~ expected number of frauds\n",
        "    mean_pred_prob = ('pred_proba','mean')\n",
        ").reset_index().sort_values('date')\n",
        "daily['fraud_rate'] = daily['actual_frauds'] / daily['total_transactions']\n",
        "\n",
        "# Save daily CSV and plot\n",
        "daily.to_csv(os.path.join(OUTPUT_DIR,'daily_agg.csv'), index=False)\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(daily['date'], daily['actual_frauds'], label='actual frauds (daily)', alpha=0.8)\n",
        "plt.plot(daily['date'], daily['expected_frauds'], label='expected_frauds (sum probs)', alpha=0.8)\n",
        "plt.legend(); plt.title('Daily frauds: actual vs expected'); plt.xlabel('date'); plt.ylabel('count')\n",
        "plt.savefig(os.path.join(OUTPUT_DIR,'daily_frauds.png'), bbox_inches='tight'); plt.close()\n",
        "\n",
        "# 8) Detect suspicious time windows (simple statistical rule + rolling)\n",
        "# flags: daily frauds above mean + 3*std OR days where fraud_rate is in top 1%\n",
        "mean_f = daily['actual_frauds'].mean(); std_f = daily['actual_frauds'].std()\n",
        "daily['anomaly_mean3std'] = daily['actual_frauds'] > (mean_f + 3*std_f)\n",
        "p99 = daily['fraud_rate'].quantile(0.99)\n",
        "daily['anomaly_p99_rate'] = daily['fraud_rate'] >= p99\n",
        "# rolling-window burst detection (7-day rolling sum)\n",
        "daily['frauds_7d'] = daily['actual_frauds'].rolling(7, min_periods=1).sum()\n",
        "thr_7d = daily['frauds_7d'].mean() + 3*daily['frauds_7d'].std()\n",
        "daily['anomaly_7d_burst'] = daily['frauds_7d'] > thr_7d\n",
        "\n",
        "daily[['date','total_transactions','actual_frauds','expected_frauds','anomaly_mean3std','anomaly_p99_rate','anomaly_7d_burst']].to_csv(os.path.join(OUTPUT_DIR,'daily_anomaly_flags.csv'), index=False)\n",
        "\n",
        "# 9) Forecasting daily fraud counts (two options)\n",
        "# Option A: try SARIMAX / ARIMA (requires statsmodels)\n",
        "forecast_df = None\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    # Fit a simple SARIMAX(1,0,1) without seasonal terms for speed\n",
        "    daily_idx = daily.set_index('date')['actual_frauds']\n",
        "    model = sm.tsa.SARIMAX(daily_idx, order=(1,0,1), enforce_stationarity=False, enforce_invertibility=False)\n",
        "    res = model.fit(disp=False)\n",
        "    fc = res.get_forecast(steps=FORECAST_DAYS)\n",
        "    fc_mean = fc.predicted_mean\n",
        "    fc_ci = fc.conf_int(alpha=0.05)\n",
        "    forecast_df = pd.DataFrame({\n",
        "        'date': fc_mean.index,\n",
        "        'predicted_frauds': fc_mean.values,\n",
        "        'lower': fc_ci.iloc[:,0].values,\n",
        "        'upper': fc_ci.iloc[:,1].values\n",
        "    })\n",
        "    print(\"Used SARIMAX forecasting (statsmodels).\")\n",
        "except Exception as e:\n",
        "    # Option B: robust fallback â€” linear trend on day index + hold MA(7) baseline\n",
        "    print(\"SARIMAX not available or failed:\", str(e))\n",
        "    daily = daily.reset_index(drop=True)\n",
        "    daily['day_idx'] = (daily['date'] - daily['date'].min()).dt.days\n",
        "    # Linear regression on actual_frauds\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    lr_time = LinearRegression()\n",
        "    lr_time.fit(daily[['day_idx']], daily['actual_frauds'])\n",
        "    future_dates = pd.date_range(daily['date'].max() + pd.Timedelta(days=1), periods=FORECAST_DAYS)\n",
        "    future_idx = ((future_dates - daily['date'].min()).days).astype(int)\n",
        "    preds = lr_time.predict(future_idx.reshape(-1,1))\n",
        "    resid_std = (daily['actual_frauds'] - lr_time.predict(daily[['day_idx']])).std()\n",
        "    lower = preds - 1.96*resid_std\n",
        "    upper = preds + 1.96*resid_std\n",
        "    preds = np.clip(preds, 0, None); lower = np.clip(lower, 0, None)\n",
        "    forecast_df = pd.DataFrame({'date': future_dates, 'predicted_frauds': preds, 'lower': lower, 'upper': upper})\n",
        "\n",
        "# Save forecast and plot\n",
        "forecast_df.to_csv(os.path.join(OUTPUT_DIR,'forecast_365d.csv'), index=False)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(daily['date'], daily['actual_frauds'], label='actual daily frauds')\n",
        "plt.plot(forecast_df['date'], forecast_df['predicted_frauds'], label='forecast next 365d', color='C1')\n",
        "plt.fill_between(forecast_df['date'], forecast_df['lower'], forecast_df['upper'], color='C1', alpha=0.2)\n",
        "plt.legend(); plt.title('Daily frauds and 1-year forecast'); plt.savefig(os.path.join(OUTPUT_DIR,'forecast_plot.png'), bbox_inches='tight'); plt.close()\n",
        "\n",
        "# 10) Save scored transactions (top suspicious transactions)\n",
        "df = df.sort_values('pred_proba', ascending=False)\n",
        "df[['datetime','Amount','Class','pred_proba']].head(200).to_csv(os.path.join(OUTPUT_DIR,'top_200_suspects.csv'), index=False)\n",
        "\n",
        "print(\"All outputs saved to\", OUTPUT_DIR)\n",
        "print(\"Top 5 days flagged as anomalies (by mean+3std):\")\n",
        "print(daily[daily['anomaly_mean3std']].sort_values('actual_frauds', ascending=False).head(10))"
      ]
    }
  ]
}